{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8016497",
   "metadata": {},
   "source": [
    "These are all the packages you‚Äôll need for Python statistics calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557ceb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c76ba3",
   "metadata": {},
   "source": [
    "Let‚Äôs create some data to work with. You‚Äôll start with Python lists that contain some arbitrary numeric data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ebed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [8.0, 1, 2.5, 4, 28.0]\n",
    "\n",
    "x_with_nan = [8.0, 1, 2.5, math.nan, 4, 28.0]\n",
    "\n",
    "#x_with_nan contains a nan value. It‚Äôs important \n",
    "#to understand the behavior of the Python statistics routines when they come across a not-a-number value (nan). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd9952",
   "metadata": {},
   "source": [
    " How do you get a nan value?\n",
    "\n",
    "In Python, you can use any of the following:\n",
    "\n",
    "- float('nan')\n",
    "- math.nan\n",
    "- np.nan\n",
    "\n",
    "\n",
    "### Now, create np.ndarray and pd.Series objects that correspond to x and x_with_nan:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b87ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, y_with_nan = np.array(x), np.array(x_with_nan)\n",
    "\n",
    "\n",
    "z, z_with_nan = pd.Series(x), pd.Series(x_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97279b86",
   "metadata": {},
   "source": [
    "#### Measures of Central Tendency\n",
    "\n",
    "The measures of central tendency show the central or middle values of datasets. There are several definitions of what‚Äôs considered to be the center of a dataset. In this tutorial, you‚Äôll learn how to identify and calculate these measures of central tendency:\n",
    "\n",
    "- Mean\n",
    "- Weighted mean\n",
    "- Geometric mean\n",
    "- Harmonic mean\n",
    "- Median\n",
    "- Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d4ac0",
   "metadata": {},
   "source": [
    "**Mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2431d537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8.7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8.7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8.7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8.7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8.7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can calculate the mean with pure Python using sum() and len(), without importing libraries:\n",
    "mean_ = sum(x) / len(x)\n",
    "mean_\n",
    "\n",
    "\n",
    "#Although this is clean and elegant, you can also apply built-in Python statistics functions:\n",
    "\n",
    "mean_=statistics.mean(x)\n",
    "mean_\n",
    "\n",
    "#If you use NumPy, then you can get the mean with np.mean():\n",
    "\n",
    "mean_=np.mean(y)\n",
    "mean_\n",
    "\n",
    "mean_ = y.mean()\n",
    "mean_\n",
    "\n",
    "#The function mean() and method .mean() from NumPy return the same result as statistics.mean(). \n",
    "#This is also the case when there are nan values among your data:\n",
    "\n",
    "np.mean(y_with_nan)\n",
    "\n",
    "y_with_nan.mean()\n",
    "\n",
    "\n",
    "#If you prefer to ignore nan values, then you can use np.nanmean():\n",
    "np.nanmean(y_with_nan)\n",
    "\n",
    "\n",
    "\n",
    "#pd.Series objects also have the method .mean():As you can see, it‚Äôs used similarly as in the case of NumPy. \n",
    "#However, .mean() from Pandas ignores nan values by default:\n",
    "z_with_nan.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94314751",
   "metadata": {},
   "source": [
    "#### Weighted Mean\n",
    "\n",
    "The weighted mean, also called the weighted arithmetic mean or weighted average, is a generalization of the arithmetic mean that enables you to define the relative contribution of each data point to the result.\n",
    "\n",
    "The weighted mean is very handy when you need the mean of a dataset containing items that occur with given relative frequencies. For example, say that you have a set in which 20% of all items are equal to 2, 50% of the items are equal to 4, and the remaining 30% of the items are equal to 8. You can calculate the mean of such a set like this:\n",
    "\n",
    "\n",
    "0.2 * 2 + 0.5 * 4 + 0.3 * 8\n",
    "4.8\n",
    "\n",
    "\n",
    "You can implement the weighted mean in pure Python by combining sum() with either range() or zip():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e0bd3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6.95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [8.0, 1, 2.5, 4, 28.0]\n",
    "\n",
    "w = [0.1, 0.2, 0.3, 0.25, 0.15]\n",
    "\n",
    "\n",
    "wmean = sum(w[i] * x[i] for i in range(len(x))) / sum(w)\n",
    "\n",
    "wmean\n",
    "\n",
    "#or\n",
    "\n",
    "wmean = sum(x_ * w_ for (x_, w_) in zip(x, w)) / sum(w)\n",
    "\n",
    "wmean\n",
    "\n",
    "\n",
    "#However, if you have large datasets, then NumPy is likely to provide a better solution. \n",
    "#You can use np.average() to get the weighted mean of NumPy arrays or Pandas Series:\n",
    "\n",
    "\n",
    "y, z, w = np.array(x), pd.Series(x), np.array(w)\n",
    "\n",
    "wmean = np.average(y, weights=w)\n",
    "\n",
    "wmean\n",
    "\n",
    "#or\n",
    "\n",
    "wmean = np.average(z, weights=w)\n",
    "\n",
    "wmean\n",
    "\n",
    "\n",
    "#Another solution is to use the element-wise product w * y with np.sum() or .sum():\n",
    "\n",
    "(w * y).sum() / w.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e880ab",
   "metadata": {},
   "source": [
    "### Harmonic Mean\n",
    "\n",
    "The harmonic mean is the reciprocal of the mean of the reciprocals of all items in the dataset: ùëõ / Œ£·µ¢(1/ùë•·µ¢), where ùëñ = 1, 2, ‚Ä¶, ùëõ and ùëõ is the number of items in the dataset ùë•. One variant of the pure Python implementation of the harmonic mean is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bee3fa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7613412228796843"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.7613412228796843"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.7613412228796843"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.7613412228796843"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmean = len(x) / sum(1 / item for item in x)\n",
    "\n",
    "hmean\n",
    "\n",
    "#or\n",
    "\n",
    "hmean = statistics.harmonic_mean(x)\n",
    "\n",
    "hmean\n",
    "\n",
    "#A third way to calculate the harmonic mean is to use scipy.stats.hmean():\n",
    "\n",
    "scipy.stats.hmean(y)\n",
    "\n",
    "scipy.stats.hmean(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2667ecb",
   "metadata": {},
   "source": [
    "#### Geometric Mean\n",
    "\n",
    "The geometric mean is the ùëõ-th root of the product of all ùëõ elements ùë•·µ¢ in a dataset ùë•: ‚Åø‚àö(Œ†·µ¢ùë•·µ¢), where ùëñ = 1, 2, ‚Ä¶, ùëõ. The following figure illustrates the arithmetic, harmonic, and geometric means of a dataset:\n",
    "\n",
    "As you can see, the value of the geometric mean, in this case, differs significantly from the values of the arithmetic (8.7) and harmonic (2.76) means for the same dataset x.\n",
    "\n",
    "Python 3.8 introduced statistics.geometric_mean(), which converts all values to floating-point numbers and returns their geometric mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40de3cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.677885674856041"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.67788567485604"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.67788567485604"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.67788567485604"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmean = 1\n",
    "\n",
    "for item in x:\n",
    "     gmean *= item\n",
    "\n",
    "gmean **= 1 / len(x)\n",
    "\n",
    "gmean\n",
    "\n",
    "\n",
    "\n",
    "#or\n",
    "\n",
    "gmean = statistics.geometric_mean(x)\n",
    "\n",
    "gmean\n",
    "\n",
    "\n",
    "## You can also get the geometric mean with scipy.stats.gmean():\n",
    "\n",
    "scipy.stats.gmean(y)\n",
    "\n",
    "scipy.stats.gmean(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e16bf9",
   "metadata": {},
   "source": [
    "### Median\n",
    "\n",
    "The sample median is the middle element of a sorted dataset. The dataset can be sorted in increasing or decreasing order. If the number of elements ùëõ of the dataset is odd, then the median is the value at the middle position: 0.5(ùëõ + 1). If ùëõ is even, then the median is the arithmetic mean of the two values in the middle, that is, the items at the positions 0.5ùëõ and 0.5ùëõ + 1.\n",
    "\n",
    "For example, if you have the data points 2, 4, 1, 8, and 9, then the median value is 4, which is in the middle of the sorted dataset (1, 2, 4, 8, 9). If the data points are 2, 4, 1, and 8, then the median is 3, which is the average of the two middle elements of the sorted sequence (2 and 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6291dc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(x)\n",
    "if n % 2:\n",
    "    median_ = sorted(x)[round(0.5*(n-1))]\n",
    "else:\n",
    "    x_ord, index = sorted(x), round(0.5 * n)\n",
    "    median_ = 0.5 * (x_ord[index-1] + x_ord[index])\n",
    "\n",
    "median_\n",
    "\n",
    "\n",
    "\n",
    "#Two most important steps of this implementation are as follows:\n",
    "\n",
    "    #Sorting the elements of the dataset\n",
    "    #Finding the middle element(s) in the sorted dataset\n",
    "\n",
    "    \n",
    "#You can get the median with statistics.median():\n",
    "\n",
    "median_ = statistics.median(x)\n",
    "median_\n",
    "\n",
    "median_ = statistics.median(x[:-1])\n",
    "\n",
    "median_\n",
    "\n",
    "# The sorted version of x is [1, 2.5, 4, 8.0, 28.0], so the element in the middle is 4. \n",
    "#The sorted version of x[:-1], which is x without the last item 28.0, is [1, 2.5, 4, 8.0]. \n",
    "# Now, there are two middle elements, 2.5 and 4. Their average is 3.25.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0660ba0",
   "metadata": {},
   "source": [
    "median_low() and median_high() are two more functions related to the median in the Python statistics library. They always return an element from the dataset:\n",
    "\n",
    "If the number of elements is odd, then there‚Äôs a single middle value, so these functions behave just like median().\n",
    "\n",
    "If the number of elements is even, then there are two middle values. In this case, median_low() returns the lower and median_high() the higher middle value.\n",
    "\n",
    "You can use these functions just as you‚Äôd use median():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd7ac76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.median_low(x[:-1])\n",
    "\n",
    "statistics.median_high(x[:-1])\n",
    "\n",
    "# Again, the sorted version of x[:-1] is [1, 2.5, 4, 8.0]. The two elements in the middle are 2.5 (low) and 4 (high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dc1c96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.25"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unlike most other functions from the Python statistics library, median(), median_low(), and median_high()\n",
    "#don‚Äôt return nan when there are nan values among the data points:\n",
    "\n",
    "statistics.median(x_with_nan)\n",
    "\n",
    "statistics.median_low(x_with_nan)\n",
    "\n",
    "statistics.median_high(x_with_nan)\n",
    "\n",
    "\n",
    "#Beware of this behavior because it might not be what you want!\n",
    "\n",
    "#You can also get the median with np.median():\n",
    "\n",
    "median_ = np.median(y)\n",
    "\n",
    "median_\n",
    "\n",
    "median_ = np.median(y[:-1])\n",
    "\n",
    "median_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ef20f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.25"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#However, if there‚Äôs a nan value in your dataset, then np.median() issues the RuntimeWarning and returns nan. \n",
    "#If this behavior is not what you want, then you can use nanmedian() to ignore all nan values:\n",
    "\n",
    "np.nanmedian(y_with_nan)\n",
    "\n",
    "np.nanmedian(y_with_nan[:-1])\n",
    "\n",
    "\n",
    "#Pandas Series objects have the method .median() that ignores nan values by default:\n",
    "\n",
    "z.median()\n",
    "\n",
    "z_with_nan.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4e648",
   "metadata": {},
   "source": [
    "### Mode\n",
    "\n",
    "The sample mode is the value in the dataset that occurs most frequently. If there isn‚Äôt a single such value, then the set is multimodal since it has multiple modal values. For example, in the set that contains the points 2, 3, 2, 8, and 12, the number 2 is the mode because it occurs twice, unlike the other items that occur only once.\n",
    "\n",
    "This is how you can get the mode with pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94d42d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{2, 3, 8, 12}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = [2, 3, 2, 8, 12]\n",
    "\n",
    "mode_ = max((u.count(item), item) for item in set(u))[1]\n",
    "\n",
    "mode_\n",
    "\n",
    "#You use u.count() to get the number of occurrences of each item in u\n",
    "\n",
    "u.count(12)\n",
    "\n",
    "set(u)\n",
    "# set(u) returns a Python set with all unique items in u. \n",
    "# You can use this trick to optimize working with larger data, especially when you expect to see a lot of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2bf36a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[12, 15]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_ = statistics.mode(u)\n",
    "\n",
    "mode_\n",
    "\n",
    "mode_ = statistics.multimode(u)\n",
    "\n",
    "mode_\n",
    "\n",
    "\n",
    "#As you can see, mode() returned a single value, while multimode() returned the list that contains the result. \n",
    "#This isn‚Äôt the only difference between the two functions, though. \n",
    "#If there‚Äôs more than one modal value, then mode() raises StatisticsError, \n",
    "#while multimode() returns the list with all modes:\n",
    "\n",
    "v = [12, 15, 12, 15, 21, 15, 12]\n",
    "\n",
    "statistics.mode(v)  # Raises StatisticsError\n",
    "\n",
    "statistics.multimode(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d800db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[nan]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nan\n",
    "\n",
    "statistics.mode([2, math.nan, 2])\n",
    "\n",
    "statistics.multimode([2, math.nan, 2])\n",
    "\n",
    "statistics.mode([2, math.nan, 0, math.nan, 5])\n",
    "\n",
    "statistics.multimode([2, math.nan, 0, math.nan, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15084dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModeResult(mode=array([2]), count=array([2]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ModeResult(mode=array([12]), count=array([3]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can also get the mode with scipy.stats.mode():\n",
    "\n",
    "u, v = np.array(u), np.array(v)\n",
    "\n",
    "mode_ = scipy.stats.mode(u)\n",
    "\n",
    "mode_\n",
    "\n",
    "mode_ = scipy.stats.mode(v)\n",
    "\n",
    "mode_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "292cddff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can get the mode and its number of occurrences as NumPy arrays with dot notation:\n",
    "\n",
    "mode_.mode\n",
    "\n",
    "mode_.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92c6ff67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "dtype: int32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    12\n",
       "1    15\n",
       "dtype: int32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    2.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pandas Series objects have the method .mode() that handles multimodal values well and ignores nan values by default:\n",
    "\n",
    "u, v, w = pd.Series(u), pd.Series(v), pd.Series([2, 2, math.nan])\n",
    "\n",
    "u.mode()\n",
    "\n",
    "\n",
    "\n",
    "v.mode()\n",
    "\n",
    "\n",
    "\n",
    "w.mode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7d425",
   "metadata": {},
   "source": [
    "### Measures of Variability\n",
    "\n",
    "The measures of central tendency aren‚Äôt sufficient to describe data. You‚Äôll also need the measures of variability that quantify the spread of data points. In this section, you‚Äôll learn how to identify and calculate the following variability measures:\n",
    "\n",
    "- Variance\n",
    "- Standard deviation\n",
    "- Skewness\n",
    "- Percentiles\n",
    "- Ranges\n",
    "- Variance\n",
    "\n",
    "**The sample variance quantifies the spread of the data. It shows numerically how far the data points are from the mean.** You can express the sample variance of the dataset ùë• with ùëõ elements mathematically as ùë†¬≤ = Œ£·µ¢(ùë•·µ¢ ‚àí mean(ùë•))¬≤ / (ùëõ ‚àí 1), where ùëñ = 1, 2, ‚Ä¶, ùëõ and mean(ùë•) is the sample mean of ùë•. If you want to understand deeper why you divide the sum with ùëõ ‚àí 1 instead of ùëõ, then you can dive deeper into Bessel‚Äôs correction.\n",
    "\n",
    "The following figure shows you why it‚Äôs important to consider the variance when describing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "168217e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.19999999999999"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "123.2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "123.19999999999999"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "123.19999999999999"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here‚Äôs how you can calculate the sample variance with pure Python:\n",
    "\n",
    "n = len(x)\n",
    "\n",
    "mean_ = sum(x) / n\n",
    "\n",
    "var_ = sum((item - mean_)**2 for item in x) / (n - 1)\n",
    "\n",
    "var_\n",
    "\n",
    "\n",
    "#This approach is sufficient and calculates the sample variance well. \n",
    "#However, the shorter and more elegant solution is to call the existing function statistics.variance():\n",
    "\n",
    "var_ = statistics.variance(x)\n",
    "\n",
    "var_\n",
    "\n",
    "\n",
    "#If you have nan values among your data, then statistics.variance() will return nan:\n",
    "\n",
    "statistics.variance(x_with_nan)\n",
    "\n",
    "\n",
    "#You can also calculate the sample variance with NumPy. \n",
    "#You should use the function np.var() or the corresponding method .var():\n",
    "\n",
    "var_ = np.var(y, ddof=1)\n",
    "\n",
    "var_\n",
    "\n",
    "var_ = y.var(ddof=1)\n",
    "\n",
    "var_\n",
    "\n",
    "# It‚Äôs very important to specify the parameter ddof=1. That‚Äôs how you set the delta degrees of freedom to 1. \n",
    "# This parameter allows the proper calculation of ùë†¬≤, with (ùëõ ‚àí 1) in the denominator instead of ùëõ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "437a91ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "123.19999999999999"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you have nan values in the dataset, then np.var() and .var() will return nan:\n",
    "\n",
    "np.var(y_with_nan, ddof=1)\n",
    "\n",
    "\n",
    "y_with_nan.var(ddof=1)\n",
    "\n",
    "\n",
    "#This is consistent with np.mean() and np.average(). If you want to skip nan values, then you should use np.nanvar():\n",
    "np.nanvar(y_with_nan, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4bf3b51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.19999999999999"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "123.19999999999999"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.Series objects have the method .var() that skips nan values by default:\n",
    "\n",
    "z.var(ddof=1)\n",
    "\n",
    "z_with_nan.var(ddof=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b8008",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "The sample standard deviation is another measure of data spread. It‚Äôs connected to the sample variance, as standard deviation, ùë†, is the positive square root of the sample variance. The standard deviation is often more convenient than the variance because it has the same unit as the data points. Once you get the variance, you can calculate the standard deviation with pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9efedce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.099549540409285"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_ = var_ ** 0.5\n",
    "\n",
    "std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dab71a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.099549540409287"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Although this solution works, you can also use statistics.stdev():\n",
    "\n",
    "std_ = statistics.stdev(x)\n",
    "\n",
    "std_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb49c9",
   "metadata": {},
   "source": [
    "You can get the standard deviation with NumPy in almost the same way. You can use the function std() and the corresponding method .std() to calculate the standard deviation. If there are nan values in the dataset, then they‚Äôll return nan. To ignore nan values, you should use np.nanstd(). You use std(), .std(), and nanstd() from NumPy as you would use var(), .var(), and nanvar():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d22aa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.099549540409285"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11.099549540409285"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11.099549540409285"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y, ddof=1)\n",
    "\n",
    "y.std(ddof=1)\n",
    "\n",
    "np.std(y_with_nan, ddof=1)\n",
    "\n",
    "y_with_nan.std(ddof=1)\n",
    "\n",
    "np.nanstd(y_with_nan, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5411fd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.099549540409285"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11.099549540409285"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Don‚Äôt forget to set the delta degrees of freedom to 1!\n",
    "\n",
    "#pd.Series objects also have the method .std() that skips nan by default:\n",
    "\n",
    "z.std(ddof=1)\n",
    "\n",
    "z_with_nan.std(ddof=1)\n",
    "\n",
    "# The parameter ddof defaults to 1, so you can omit it.\n",
    "# Again, if you want to treat nan values differently, then apply the parameter skipna.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08713978",
   "metadata": {},
   "source": [
    "### Skewness\n",
    "\n",
    "The sample skewness measures the asymmetry of a data sample.\n",
    "\n",
    "There are several mathematical definitions of skewness. One common expression to calculate the skewness of the dataset ùë• with ùëõ elements is (ùëõ¬≤ / ((ùëõ ‚àí 1)(ùëõ ‚àí 2))) (Œ£·µ¢(ùë•·µ¢ ‚àí mean(ùë•))¬≥ / (ùëõùë†¬≥)). A simpler expression is Œ£·µ¢(ùë•·µ¢ ‚àí mean(ùë•))¬≥ ùëõ / ((ùëõ ‚àí 1)(ùëõ ‚àí 2)ùë†¬≥), where ùëñ = 1, 2, ‚Ä¶, ùëõ and mean(ùë•) is the sample mean of ùë•. The skewness defined like this is called the adjusted Fisher-Pearson standardized moment coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b56d880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9470432273905929"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Once you‚Äôve calculated the size of your dataset n, the sample mean mean_, and the standard deviation std_, \n",
    "#you can get the sample skewness with pure Python:\n",
    "\n",
    "x = [8.0, 1, 2.5, 4, 28.0]\n",
    "\n",
    "n = len(x)\n",
    "\n",
    "mean_ = sum(x) / n\n",
    "\n",
    "var_ = sum((item - mean_)**2 for item in x) / (n - 1)\n",
    "\n",
    "std_ = var_ ** 0.5\n",
    "\n",
    "skew_ = (sum((item - mean_)**3 for item in x)\n",
    "         * n / ((n - 1) * (n - 2) * std_**3))\n",
    "skew_\n",
    "\n",
    "\n",
    "#The skewness is positive, so x has a right-side tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "825c347b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9470432273905927"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can also calculate the sample skewness with scipy.stats.skew():\n",
    "\n",
    "y, y_with_nan = np.array(x), np.array(x_with_nan)\n",
    "\n",
    "scipy.stats.skew(y, bias=False)\n",
    "\n",
    "scipy.stats.skew(y_with_nan, bias=False)\n",
    "\n",
    "#The obtained result is the same as the pure Python implementation.\n",
    "#The parameter bias is set to False to enable the corrections for statistical bias. \n",
    "#The optional parameter nan_policy can take the values 'propagate', 'raise', or 'omit'.\n",
    "#It allows you to control how you‚Äôll handle nan values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d98b3457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9470432273905924"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.9470432273905924"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pandas Series objects have the method .skew() that also returns the skewness of a dataset:\n",
    "\n",
    "z, z_with_nan = pd.Series(x), pd.Series(x_with_nan)\n",
    "\n",
    "z.skew()\n",
    "\n",
    "\n",
    "z_with_nan.skew()\n",
    "\n",
    "# Like other methods, .skew() ignores nan values by default, \n",
    "# because of the default value of the optional parameter skipna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae20e5",
   "metadata": {},
   "source": [
    "### Percentiles\n",
    "\n",
    "The sample ùëù percentile is the element in the dataset such that ùëù% of the elements in the dataset are less than or equal to that value. Also, (100 ‚àí ùëù)% of the elements are greater than or equal to that value. If there are two such elements in the dataset, then the sample ùëù percentile is their arithmetic mean. Each dataset has three quartiles, which are the percentiles that divide the dataset into four parts:\n",
    "\n",
    "The first quartile is the sample 25th percentile. It divides roughly 25% of the smallest items from the rest of the dataset.\n",
    "\n",
    "The second quartile is the sample 50th percentile or the median. Approximately 25% of the items lie between the first and second quartiles and another 25% between the second and third quartiles.\n",
    "\n",
    "The third quartile is the sample 75th percentile. It divides roughly 25% of the largest items from the rest of the dataset.\n",
    "Each part has approximately the same number of items. If you want to divide your data into several intervals, then you can use statistics.quantiles():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b960e7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0.1, 8.0, 21.0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [-5.0, -1.1, 0.1, 2.0, 8.0, 12.8, 21.0, 25.8, 41.0]\n",
    "\n",
    "statistics.quantiles(x, n=2)\n",
    "\n",
    "statistics.quantiles(x, n=4, method='inclusive')\n",
    "\n",
    "# In this example, 8.0 is the median of x, while 0.1 and 21.0 are the sample 25th and 75th percentiles, respectively.\n",
    "# The parameter n defines the number of resulting equal-probability percentiles, and method determines how to calculate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "992e3235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.44"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "34.919999999999995"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can also use np.percentile() to determine any sample percentile in your dataset. \n",
    "#For example, this is how you can find the 5th and 95th percentiles:\n",
    "\n",
    "y = np.array(x)\n",
    "\n",
    "np.percentile(y, 5)\n",
    "\n",
    "np.percentile(y, 95)\n",
    "\n",
    "\n",
    "# percentile() takes several arguments. \n",
    "# You have to provide the dataset as the first argument and the percentile value as the second. \n",
    "# The dataset can be in the form of a NumPy array, list, tuple, or similar data structure. \n",
    "# The percentile can be a number between 0 and 100 like in the example above, but it can also be a sequence of numbers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d144dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  8. , 21. ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-5. , -1.1,  nan,  0.1,  2. ,  8. , 12.8, 21. , 25.8, 41. ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  8. , 21. ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(y, [25, 50, 75])\n",
    "\n",
    "np.median(y)\n",
    "\n",
    "#This code calculates the 25th, 50th, and 75th percentiles all at once. \n",
    "#If the percentile value is a sequence, then percentile() returns a NumPy array with the results. \n",
    "#The first statement returns the array of quartiles. \n",
    "#The second statement returns the median, so you can confirm it‚Äôs equal to the 50th percentile, which is 8.0.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#If you want to ignore nan values, then use np.nanpercentile() instead:\n",
    "\n",
    "\n",
    "y_with_nan = np.insert(y, 2, np.nan)\n",
    "\n",
    "y_with_nan\n",
    "\n",
    "np.nanpercentile(y_with_nan, [25, 50, 75])\n",
    "\n",
    "#That‚Äôs how you can avoid nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88e7207b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.44"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "34.919999999999995"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  8. , 21. ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  8. , 21. ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NumPy also offers you very similar functionality in quantile() and nanquantile(). \n",
    "#If you use them, then you‚Äôll need to provide the quantile values as the numbers between 0 and 1 instead of percentiles:\n",
    "\n",
    "np.quantile(y, 0.05)\n",
    "\n",
    "np.quantile(y, 0.95)\n",
    "\n",
    "np.quantile(y, [0.25, 0.5, 0.75])\n",
    "\n",
    "np.nanquantile(y_with_nan, [0.25, 0.5, 0.75])\n",
    "\n",
    "\n",
    "# The results are the same as in the previous examples, \n",
    "# but here your arguments are between 0 and 1. In other words, you passed 0.05 instead of 5 and 0.95 instead of 95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0542f235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.44"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "34.919999999999995"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.25     0.1\n",
       "0.50     8.0\n",
       "0.75    21.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.25     0.1\n",
       "0.50     8.0\n",
       "0.75    21.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.Series objects have the method .quantile():\n",
    "\n",
    "z, z_with_nan = pd.Series(y), pd.Series(y_with_nan)\n",
    "\n",
    "z.quantile(0.05)\n",
    "\n",
    "z.quantile(0.95)\n",
    "\n",
    "\n",
    "\n",
    "z.quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "\n",
    "z_with_nan.quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "#.quantile() also needs you to provide the quantile value as the argument. This value can be a number between 0 and 1 or a sequence of numbers. In the first case, .quantile() returns a scalar. In the second case, it returns a new Series holding the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee676133",
   "metadata": {},
   "source": [
    "### Ranges\n",
    "\n",
    "The range of data is the difference between the maximum and minimum element in the dataset. You can get it with the function np.ptp():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51c32087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ptp(y)\n",
    "\n",
    "np.ptp(z)\n",
    "\n",
    "np.ptp(y_with_nan)\n",
    "\n",
    "np.ptp(z_with_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f94988",
   "metadata": {},
   "source": [
    "This function returns nan if there are nan values in your NumPy array. If you use a Pandas Series object, then it will return a number.\n",
    "\n",
    "Alternatively, you can use built-in Python, NumPy, or Pandas functions and methods to calculate the maxima and minima of sequences:\n",
    "\n",
    "- max() and min() from the Python standard library\n",
    "- amax() and amin() from NumPy\n",
    "- nanmax() and nanmin() from NumPy to ignore nan values\n",
    "- .max() and .min() from NumPy\n",
    "- .max() and .min() from Pandas to ignore nan values by default\n",
    "\n",
    "Here are some examples of how you would use these routines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8728954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(y) - np.amin(y)\n",
    "\n",
    "\n",
    "np.nanmax(y_with_nan) - np.nanmin(y_with_nan)\n",
    "\n",
    "y.max() - y.min()\n",
    "\n",
    "z.max() - z.min()\n",
    "\n",
    "z_with_nan.max() - z_with_nan.min()\n",
    "\n",
    "#That‚Äôs how you get the range of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fdce88d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.9"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20.9"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The interquartile range is the difference between the first and third quartile. \n",
    "#Once you calculate the quartiles, you can take their difference:\n",
    "\n",
    "quartiles = np.quantile(y, [0.25, 0.75])\n",
    "quartiles[1] - quartiles[0]\n",
    "\n",
    "\n",
    "\n",
    "quartiles = z.quantile([0.25, 0.75])\n",
    "quartiles[0.75] - quartiles[0.25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236f38b",
   "metadata": {},
   "source": [
    "### Summary of Descriptive Statistics\n",
    "\n",
    "SciPy and Pandas offer useful routines to quickly get descriptive statistics with a single function or method call. You can use scipy.stats.describe() like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da377afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=9, minmax=(-5.0, 41.0), mean=11.622222222222222, variance=228.75194444444446, skewness=0.9249043136685094, kurtosis=0.14770623629658886)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = scipy.stats.describe(y, ddof=1, bias=False)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e50e1",
   "metadata": {},
   "source": [
    "describe() returns an object that holds the following descriptive statistics:\n",
    "\n",
    "- nobs: the number of observations or elements in your dataset\n",
    "- minmax: the tuple with the minimum and maximum values of your dataset\n",
    "- mean: the mean of your dataset\n",
    "- variance: the variance of your dataset\n",
    "- skewness: the skewness of your dataset\n",
    "- kurtosis: the kurtosis of your dataset\n",
    "\n",
    "You can access particular values with dot notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc36cec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-5.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "41.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11.622222222222222"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "228.75194444444446"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9249043136685094"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.14770623629658886"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.nobs\n",
    "\n",
    "result.minmax[0]  # Min\n",
    "\n",
    "result.minmax[1]  # Max\n",
    "\n",
    "result.mean\n",
    "\n",
    "result.variance\n",
    "\n",
    "result.skewness\n",
    "\n",
    "result.kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6efd595f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     9.000000\n",
       "mean     11.622222\n",
       "std      15.124548\n",
       "min      -5.000000\n",
       "25%       0.100000\n",
       "50%       8.000000\n",
       "75%      21.000000\n",
       "max      41.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11.622222222222222"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15.12454774346805"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-5.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "41.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pandas\n",
    "\n",
    "result = z.describe()\n",
    "result\n",
    "\n",
    "\n",
    "result['mean']\n",
    "\n",
    "result['std']\n",
    "\n",
    "result['min']\n",
    "\n",
    "result['max']\n",
    "\n",
    "result['25%']\n",
    "\n",
    "result['50%']\n",
    "\n",
    "result['75%']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3ef20",
   "metadata": {},
   "source": [
    "### Measures of Correlation Between Pairs of Data\n",
    "\n",
    "You‚Äôll often need to examine the relationship between the corresponding elements of two variables in a dataset. Say there are two variables, ùë• and ùë¶, with an equal number of elements, ùëõ. Let ùë•‚ÇÅ from ùë• correspond to ùë¶‚ÇÅ from ùë¶, ùë•‚ÇÇ from ùë• to ùë¶‚ÇÇ from ùë¶, and so on. You can then say that there are ùëõ pairs of corresponding elements: (ùë•‚ÇÅ, ùë¶‚ÇÅ), (ùë•‚ÇÇ, ùë¶‚ÇÇ), and so on.\n",
    "\n",
    "You‚Äôll see the following measures of correlation between pairs of data:\n",
    "\n",
    "Positive correlation exists when larger values of ùë• correspond to larger values of ùë¶ and vice versa.\n",
    "Negative correlation exists when larger values of ùë• correspond to smaller values of ùë¶ and vice versa.\n",
    "Weak or no correlation exists if there is no such apparent relationship.\n",
    "\n",
    "\n",
    "\n",
    "The two statistics that measure the correlation between datasets are covariance and the correlation coefficient. Let‚Äôs define some data to work with these measures. You‚Äôll create two Python lists and use them to get corresponding NumPy arrays and Pandas Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "80dce608",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(-10, 11))\n",
    "\n",
    "y = [0, 2, 2, 2, 2, 3, 3, 6, 7, 4, 7, 6, 6, 9, 4, 5, 5, 10, 11, 12, 14]\n",
    "\n",
    "x_, y_ = np.array(x), np.array(y)\n",
    "\n",
    "x__, y__ = pd.Series(x_), pd.Series(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b84c0c",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "The sample covariance is a measure that quantifies the strength and direction of a relationship between a pair of variables:\n",
    "\n",
    "If the correlation is positive, then the covariance is positive, as well. A stronger relationship corresponds to a higher value of the covariance.\n",
    "If the correlation is negative, then the covariance is negative, as well. A stronger relationship corresponds to a lower (or higher absolute) value of the covariance.\n",
    "If the correlation is weak, then the covariance is close to zero.\n",
    "The covariance of the variables ùë• and ùë¶ is mathematically defined as ùë†À£ ∏ = Œ£·µ¢ (ùë•·µ¢ ‚àí mean(ùë•)) (ùë¶·µ¢ ‚àí mean(ùë¶)) / (ùëõ ‚àí 1), where ùëñ = 1, 2, ‚Ä¶, ùëõ, mean(ùë•) is the sample mean of ùë•, and mean(ùë¶) is the sample mean of ùë¶. It follows that the covariance of two identical variables is actually the variance: ùë†À£À£ = Œ£·µ¢(ùë•·µ¢ ‚àí mean(ùë•))¬≤ / (ùëõ ‚àí 1) = (ùë†À£)¬≤ and ùë† ∏ ∏ = Œ£·µ¢(ùë¶·µ¢ ‚àí mean(ùë¶))¬≤ / (ùëõ ‚àí 1) = (ùë† ∏)¬≤.\n",
    "\n",
    "This is how you can calculate the covariance in pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca1e5f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.95"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(x)\n",
    "\n",
    "mean_x, mean_y = sum(x) / n, sum(y) / n\n",
    "\n",
    "cov_xy = (sum((x[k] - mean_x) * (y[k] - mean_y) for k in range(n))\n",
    "         / (n - 1))\n",
    "cov_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a49200a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38.5       , 19.95      ],\n",
       "       [19.95      , 13.91428571]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NumPy has the function cov() that returns the covariance matrix:\n",
    "\n",
    "cov_matrix = np.cov(x_, y_)\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b99694d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.5"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13.914285714285711"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that cov() has the optional parameters bias, which defaults to False, and ddof, which defaults to None. \n",
    "# Their default values are suitable for getting the sample covariance matrix. \n",
    "# The upper-left element of the covariance matrix is the covariance of x and x, or the variance of x. \n",
    "# Similarly, the lower-right element is the covariance of y and y, or the variance of y. \n",
    "# You can check to see that this is true:\n",
    "\n",
    "x_.var(ddof=1)\n",
    "\n",
    "y_.var(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eddc2f",
   "metadata": {},
   "source": [
    "As you can see, the variances of x and y are equal to cov_matrix[0, 0] and cov_matrix[1, 1], respectively.\n",
    "\n",
    "The other two elements of the covariance matrix are equal and represent the actual covariance between x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d67d3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.95"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19.95"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_xy = cov_matrix[0, 1]\n",
    "cov_xy\n",
    "\n",
    "cov_xy = cov_matrix[1, 0]\n",
    "cov_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946a92c",
   "metadata": {},
   "source": [
    "You‚Äôve obtained the same value of the covariance with np.cov() as with pure Python.\n",
    "\n",
    "Pandas Series have the method .cov() that you can use to calculate the covariance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25362811",
   "metadata": {},
   "source": [
    "### Correlation Coefficient\n",
    "\n",
    "The correlation coefficient, or Pearson product-moment correlation coefficient, is denoted by the symbol ùëü. The coefficient is another measure of the correlation between data. You can think of it as a standardized covariance. Here are some important facts about it:\n",
    "\n",
    "The value ùëü > 0 indicates positive correlation.\n",
    "The value ùëü < 0 indicates negative correlation.\n",
    "\n",
    "The value r = 1 is the maximum possible value of ùëü. It corresponds to a perfect positive linear relationship between variables.\n",
    "\n",
    "The value r = ‚àí1 is the minimum possible value of ùëü. It corresponds to a perfect negative linear relationship between variables.\n",
    "\n",
    "The value r ‚âà 0, or when ùëü is around zero, means that the correlation between variables is weak.\n",
    "The mathematical formula for the correlation coefficient is ùëü = ùë†À£ ∏ / (ùë†À£ùë† ∏) where ùë†À£ and ùë† ∏ are the standard deviations of ùë• and ùë¶ respectively. If you have the means (mean_x and mean_y) and standard deviations (std_x, std_y) for the datasets x and y, as well as their covariance cov_xy, then you can calculate the correlation coefficient with pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7bdf8ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.861950005631606"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_x = sum((item - mean_x)**2 for item in x) / (n - 1)\n",
    "\n",
    "var_y = sum((item - mean_y)**2 for item in y) / (n - 1)\n",
    "\n",
    "std_x, std_y = var_x ** 0.5, var_y ** 0.5\n",
    "\n",
    "r = cov_xy / (std_x * std_y)\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2c73874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.861950005631606"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5.122760847201171e-07"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You‚Äôve got the variable r that represents the correlation coefficient.\n",
    "\n",
    "# scipy.stats has the routine pearsonr() that calculates the correlation coefficient and the ùëù-value:\n",
    "\n",
    "r, p = scipy.stats.pearsonr(x_, y_)\n",
    "\n",
    "r\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3ab21",
   "metadata": {},
   "source": [
    "pearsonr() returns a tuple with two numbers. The first one is ùëü and the second is the ùëù-value.\n",
    "\n",
    "Similar to the case of the covariance matrix, you can apply np.corrcoef() with x_ and y_ as the arguments and get the correlation coefficient matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca499b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.86195001],\n",
       "       [0.86195001, 1.        ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = np.corrcoef(x_, y_)\n",
    "\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce94a2f",
   "metadata": {},
   "source": [
    "The upper-left element is the correlation coefficient between x_ and x_. The lower-right element is the correlation coefficient between y_ and y_. Their values are equal to 1.0. The other two elements are equal and represent the actual correlation coefficient between x_ and y_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "73471ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8619500056316061"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.861950005631606"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = corr_matrix[0, 1]\n",
    "r\n",
    "\n",
    "r = corr_matrix[1, 0]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a91ce536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinregressResult(slope=0.5181818181818181, intercept=5.714285714285714, rvalue=0.861950005631606, pvalue=5.122760847201164e-07, stderr=0.06992387660074979, intercept_stderr=0.4234100995002589)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Of course, the result is the same as with pure Python and pearsonr().\n",
    "\n",
    "#You can get the correlation coefficient with scipy.stats.linregress():\n",
    "\n",
    "scipy.stats.linregress(x_, y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a60a62",
   "metadata": {},
   "source": [
    "linregress() takes x_ and y_, performs linear regression, and returns the results. slope and intercept define the equation of the regression line, while rvalue is the correlation coefficient. To access particular values from the result of linregress(), including the correlation coefficient, use dot notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ebacec53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.861950005631606"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = scipy.stats.linregress(x_, y_)\n",
    "r = result.rvalue\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "380cc77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8619500056316061"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.861950005631606"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#That‚Äôs how you can perform linear regression and obtain the correlation coefficient.\n",
    "\n",
    "#Pandas Series have the method .corr() for calculating the correlation coefficient:\n",
    "\n",
    "r = x__.corr(y__)\n",
    "r\n",
    "\n",
    "r = y__.corr(x__)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323effb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
